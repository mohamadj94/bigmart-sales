# -*- coding: utf-8 -*-
"""bigmart-sales.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UJYgoLypVGaK62UXx_I-fcLnvfu2uGYQ
"""

!pip install -q kaggle

!mkdir ~/.kaggle/

!cp '/content/drive/MyDrive/Colab Notebooks/kaggle/kaggle.json' ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets list

!kaggle datasets download -d uniabhi/bigmart-sales-data

!unzip /content/bigmart-sales-data.zip -d bigmart-sales

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression , Ridge , Lasso
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
plt.style.use('ggplot')
# pd.set_option('max_columns', 200)

"""**Train EDA &** **prerpocessing**"""

df = pd.read_csv('/content/bigmart-sales/Train.csv')
df.sample(5)

df = df[[
    # 'Item_Identifier',
    'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',
       'Item_Type',
    'Item_MRP',
    #  'Outlet_Identifier',
       'Outlet_Establishment_Year', 'Outlet_Size',
     'Outlet_Location_Type', 'Outlet_Type',
    'Item_Outlet_Sales'
    ]].copy()

df.shape

df.info()

df.describe()

df.dtypes

df.loc[df.duplicated()]

# find duplicated value
df.duplicated().sum()

# find of data null for clean datd
df.isna().sum()

# fillna for data (Replace NaN values with a constant value)
df['Item_Weight'].fillna(df['Item_Weight'].median(), inplace=True)
df['Outlet_Size'] = df['Outlet_Size'].fillna(value='no_null')
df.isnull().sum()
df.shape
df.sample(5)

df['Item_Fat_Content']=df['Item_Fat_Content'].replace({"LF":"Low Fat", "reg":"Regular", "low fat":"Low Fat"})
df['Item_Fat_Content'].unique()

df['Item_Type'].unique()

# Feature Engineering (One hot encoding)
df['Item_Fat_Content']= pd.get_dummies(df['Item_Fat_Content'], drop_first=True) #there is no class one
df.sample(5)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df['Outlet_Type'] = encoder.fit_transform(df['Outlet_Type'])
df['Outlet_Size'] = encoder.fit_transform(df['Outlet_Size'])
df['Outlet_Location_Type'] = encoder.fit_transform(df['Outlet_Location_Type'])
df['Item_Type'] = encoder.fit_transform(df['Item_Type'])
df.describe()
df.sample(10)

"""**data preprocessing**

"""

ax = df["Outlet_Establishment_Year"].value_counts() \
  .plot(kind = 'bar', title = 'top years Item_Outlet_Sales ')
  # ax.set_xlabels('Outlet_Establishment_Year')
  # ax.set_ylabels('count')

ax = df['Item_Outlet_Sales'].plot(kind='hist', bins=20)
ax.set_title('Histogram of Item_Outlet_Sales')
ax.set_xlabel('Sales')
plt.show()

ax = df['Item_Outlet_Sales'].plot(kind='kde')
ax.set_title('Histogram of Item_Outlet_Sales')
ax.set_xlabel('Sales')
plt.show()

"""##feactur relationship
Scaterplot

Heatmap corrolation

Pairplot

Groupby comparisens
"""

ax = df.plot(kind='scatter', y='Item_Outlet_Sales', x='Item_Type')
ax.set_title('Sales & Type')
plt.show()

sns.scatterplot( y='Item_Outlet_Sales', x='Item_Visibility', hue='Outlet_Establishment_Year', data=df)

sns.pairplot(df, vars=['Item_Weight',	'Item_Fat_Content',	'Item_Visibility',	'Item_Type',	'Item_MRP',
                       'Outlet_Establishment_Year',	'Outlet_Size',	'Outlet_Location_Type',	'Outlet_Type', 'Item_Outlet_Sales'])
plt.show()

df_corr = df[['Item_Weight',	'Item_Fat_Content',	'Item_Visibility',	'Item_Type',	'Item_MRP',
                       'Outlet_Establishment_Year',	'Outlet_Size',	'Outlet_Location_Type',	'Outlet_Type', 'Item_Outlet_Sales']].dropna().corr()
df_corr

sns.heatmap(df_corr, annot=True )

"""**Train_Test_Split**"""

from sklearn.model_selection import train_test_split
X_train = df.iloc[:, :-1].values
y_train = df.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Evaluate the model on the test set
predictions = model.predict(X_test)
rmse = mean_squared_error(y_test, predictions)**0.3
print("RMSE:", rmse)

r2_score(y_test, predictions)

percent_accuracy = (1 - (rmse / y_test.mean())) * 100
print(f"Model Accuracy: {percent_accuracy}%")

